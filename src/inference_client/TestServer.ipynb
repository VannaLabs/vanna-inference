{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5db6e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import grpc\n",
    "import sys\n",
    "from concurrent import futures\n",
    "import inference_pb2_grpc \n",
    "import onnx\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "import inference_pb2 \n",
    "import ezkl\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import ezkl\n",
    "import json\n",
    "from hummingbird.ml import convert\n",
    "import ast\n",
    "import config\n",
    "import ecdsa\n",
    "#from transformers import pipeline, set_seed\n",
    "from ecdsa.keys import SigningKey, VerifyingKey\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "786b23d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceServer(inference_pb2_grpc.InferenceServicer):\n",
    "    def RunInference(self, inferenceParams, context):\n",
    "        results = self.Infer(inferenceParams.modelHash, inferenceParams.modelInput)\n",
    "        return inference_pb2.InferenceResult(tx=inferenceParams.tx, node=inferenceParams.modelHash, value=str(results))\n",
    "    \n",
    "    def Infer(self, modelHash, modelInput):\n",
    "        session = onnxruntime.InferenceSession(modelHash)\n",
    "        results = session.run(curateOutputs(session), curateInputs(session, modelInput))[-1]\n",
    "        return results[0][0]\n",
    "    \n",
    "    def RunPipeline(self, pipelineParams, context):\n",
    "        results = self.Pipeline(pipelineParams.seed, pipelineParams.pipelineName, pipelineParams.modelHash, pipelineParams.modelInput)\n",
    "        return transform_pb2.TransformationResult(tx=transformParams.tx, node=transformParams.modelHash, value=str(results))\n",
    "    \n",
    "    def Pipeline(self, seed, pipeline, model, inputs):\n",
    "        generator = pipeline(pipeline, model=model)\n",
    "        set_seed(seed)\n",
    "        return generator(inputs, max_length=50, num_return_sequences=1)[0]['generated_text'].split(\".\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7599985d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serve(port, maxWorkers):\n",
    "    server = grpc.server(futures.ThreadPoolExecutor(max_workers=maxWorkers))\n",
    "    inference_pb2_grpc.add_InferenceServicer_to_server(InferenceServer(), server)\n",
    "    server.add_insecure_port(\"[::]:\" + str(port))\n",
    "    server.start()\n",
    "    server.wait_for_termination()\n",
    "    \n",
    "def parseInput(modelInput, typeString):\n",
    "    if \"tensor\" in typeString:\n",
    "        if \"float\" in typeString:\n",
    "            return onnxruntime.OrtValue.ortvalue_from_numpy(np.array([modelInput]).astype(\"float32\"))\n",
    "        if \"string\" in typeString:\n",
    "            return onnxruntime.OrtValue.ortvalue_from_numpy(np.array([modelInput]).astype(\"string\"))\n",
    "\n",
    "def curateInputs(session, modelInput):\n",
    "    inputs = {}\n",
    "    sessionInputs = session.get_inputs()\n",
    "    for i in range(0, len(sessionInputs)):\n",
    "        param = ast.literal_eval(modelInput)[i]\n",
    "        inputs[sessionInputs[i].name] = parseInput(param, sessionInputs[i].type)\n",
    "    return inputs\n",
    "\n",
    "def curateOutputs(session):\n",
    "    outputs = []\n",
    "    for o in session.get_outputs():\n",
    "        outputs.append(o.name)\n",
    "    return outputs\n",
    "\n",
    "def sign(private_key):\n",
    "    signing_key = ecdsa.SigningKey.from_string(private_key, curve=ecdsa.SECP256k1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5160f49f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This ORT build has ['AzureExecutionProvider', 'CPUExecutionProvider'] enabled. Since ORT 1.9, you are required to explicitly set the providers parameter when instantiating InferenceSession. For example, onnxruntime.InferenceSession(..., providers=['AzureExecutionProvider', 'CPUExecutionProvider'], ...)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m data_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m cal_data_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcal_data.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m session \u001b[38;5;241m=\u001b[39m onnxruntime\u001b[38;5;241m.\u001b[39mInferenceSession(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQmXQpupTphRTeXJMEz3BCt9YUF6kikcqExxPdcVoL1BBhy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m results \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mrun(curateOutputs(session), curateInputs(session, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[[0.003, 0.005, 0.004056685]]\u001b[39m\u001b[38;5;124m\"\u001b[39m))[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(results)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:432\u001b[0m, in \u001b[0;36mInferenceSession.__init__\u001b[0;34m(self, path_or_bytes, sess_options, providers, provider_options, **kwargs)\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m fallback_error \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m# Fallback is disabled. Raise the original error.\u001b[39;00m\n\u001b[0;32m--> 432\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:419\u001b[0m, in \u001b[0;36mInferenceSession.__init__\u001b[0;34m(self, path_or_bytes, sess_options, providers, provider_options, **kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m disabled_optimizers \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisabled_optimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisabled_optimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_inference_session(providers, provider_options, disabled_optimizers)\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:451\u001b[0m, in \u001b[0;36mInferenceSession._create_inference_session\u001b[0;34m(self, providers, provider_options, disabled_optimizers)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m providers \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(available_providers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisable_fallback()\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis ORT build has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavailable_providers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m enabled. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSince ORT 1.9, you are required to explicitly set \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe providers parameter when instantiating InferenceSession. For example, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    455\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monnxruntime.InferenceSession(..., providers=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavailable_providers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, ...)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    456\u001b[0m     )\n\u001b[1;32m    458\u001b[0m session_options \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sess_options \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sess_options \u001b[38;5;28;01melse\u001b[39;00m C\u001b[38;5;241m.\u001b[39mget_default_session_options()\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_path:\n",
      "\u001b[0;31mValueError\u001b[0m: This ORT build has ['AzureExecutionProvider', 'CPUExecutionProvider'] enabled. Since ORT 1.9, you are required to explicitly set the providers parameter when instantiating InferenceSession. For example, onnxruntime.InferenceSession(..., providers=['AzureExecutionProvider', 'CPUExecutionProvider'], ...)"
     ]
    }
   ],
   "source": [
    "model_path = os.path.join('QmXQpupTphRTeXJMEz3BCt9YUF6kikcqExxPdcVoL1BBhy')\n",
    "compiled_model_path = os.path.join('network.ezkl')\n",
    "pk_path = os.path.join('test.pk')\n",
    "vk_path = os.path.join('test.vk')\n",
    "settings_path = os.path.join('settings.json')\n",
    "srs_path = os.path.join('kzg.srs')\n",
    "witness_path = os.path.join('witness.json')\n",
    "data_path = os.path.join('input.json')\n",
    "cal_data_path = os.path.join('cal_data.json')\n",
    "\n",
    "session = onnxruntime.InferenceSession(\"QmXQpupTphRTeXJMEz3BCt9YUF6kikcqExxPdcVoL1BBhy\")\n",
    "results = session.run(curateOutputs(session), curateInputs(session, \"[[0.003, 0.005, 0.004056685]]\"))[-1][0][0]\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "411ed892",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array = [[0.003, 0.005, 0.004056685]]\n",
    "\n",
    "data = dict(input_data = [data_array])\n",
    "\n",
    "json.dump(data, open(data_path, 'w'))\n",
    "\n",
    "cal_data = dict(input_data = data_array)\n",
    "\n",
    "json.dump(data, open(cal_data_path, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f243367",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to generate settings: Translating node #1 \"LinearRegressor\" Unimplemented(LinearRegressor) ToTypedTranslator",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRUST_LOG=trace\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# TODO: Dictionary outputs\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m res \u001b[38;5;241m=\u001b[39m ezkl\u001b[38;5;241m.\u001b[39mgen_settings(model_path, settings_path)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m res \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      6\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m ezkl\u001b[38;5;241m.\u001b[39mcalibrate_settings(cal_data_path, model_path, settings_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresources\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to generate settings: Translating node #1 \"LinearRegressor\" Unimplemented(LinearRegressor) ToTypedTranslator"
     ]
    }
   ],
   "source": [
    "!RUST_LOG=trace\n",
    "# TODO: Dictionary outputs\n",
    "res = ezkl.gen_settings(model_path, settings_path)\n",
    "assert res == True\n",
    "\n",
    "res = await ezkl.calibrate_settings(cal_data_path, model_path, settings_path, \"resources\")  # Optimize for resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2e6aa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
